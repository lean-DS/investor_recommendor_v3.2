# -*- coding: utf-8 -*-
"""sentiment_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4syBDwArVqLO13mtS5ges1YqkTbjfVs
"""

# sentiment_utils.py
# ---------------------------------------------------------------------
# Pure utilities for news sentiment:
# - Fetch Google News RSS per (ticker, name)
# - Score with FinBERT (label × prob → [-1, 1])
# - Time-decay aggregate with 7-day half-life (configurable)
# - Return (aggregates_df, articles_df) with the exact columns you use:
#     Aggregates: Ticker | Sentiment | Count
#     Articles:   Ticker | published_at | title | score | link
#
# NOTE: This module does NO file IO. Your collector/app can save the returned DFs.
# ---------------------------------------------------------------------

from __future__ import annotations

import os
import math
import time
import re
from datetime import datetime, timezone, timedelta
from typing import Iterable, List, Sequence, Tuple

import pandas as pd
import feedparser

# -----------------------
# Configuration (env-tunable, defaults align with your current code)
# -----------------------
FINBERT_MODEL      = os.getenv("FINBERT_MODEL", "ProsusAI/finbert")
HALF_LIFE_DAYS     = float(os.getenv("HALF_LIFE_DAYS", "7.0"))
PER_TICKER_MAX_ROWS= int(os.getenv("PER_TICKER_MAX_ROWS", "12"))
POLITE_DELAY_SEC   = float(os.getenv("POLITE_DELAY_SEC", "0.20"))
MAX_TOTAL_TEXTS    = int(os.getenv("MAX_TOTAL_TEXTS", "4000"))  # safeguard on cold starts

# Google News locale (same as your files)
DEFAULT_LOCALE = {
    "hl":  os.getenv("GN_HL",  "en-GB"),
    "gl":  os.getenv("GN_GL",  "GB"),
    "ceid":os.getenv("GN_CEID","GB:en")
}

# -----------------------
# Small helpers
# -----------------------
def _now_utc() -> datetime:
    return datetime.now(timezone.utc)

def _coerce_utc(s: pd.Series) -> pd.Series:
    try:
        out = pd.to_datetime(s, utc=True, errors="coerce")
        return out
    except Exception:
        # fallback empty series
        return pd.to_datetime([], utc=True)

_WS_RE = re.compile(r"\s+")
def _clean(s: str) -> str:
    if not isinstance(s, str):
        return ""
    return _WS_RE.sub(" ", s.replace("\u00a0", " ").replace("\u200b", " ")).strip()

def _build_query(ticker: str, name: str) -> str:
    # same spirit as your code: include both ticker + company + the word 'stock'
    t = (ticker or "").strip().upper()
    n = (name or "").strip()
    return f"{t} {n} stock".strip()

def _google_news_rss(query: str, since_hours: int, locale: dict = DEFAULT_LOCALE) -> pd.DataFrame:
    """
    Pull Google News RSS for a given query. Keep items within since_hours window when possible.
    """
    url = (
        "https://news.google.com/rss/search"
        f"?q={query}"
        f"&hl={locale.get('hl','en-GB')}"
        f"&gl={locale.get('gl','GB')}"
        f"&ceid={locale.get('ceid','GB:en')}"
    )
    feed = feedparser.parse(url)
    rows = []
    cutoff = _now_utc() - timedelta(hours=int(since_hours))
    for e in feed.entries[:100]:
        title = _clean(getattr(e, "title", ""))
        link = getattr(e, "link", "")
        published = getattr(e, "published", "") or getattr(e, "updated", "")
        # Convert to UTC if possible
        try:
            published_dt = pd.to_datetime(published, utc=True, errors="coerce")
        except Exception:
            published_dt = pd.NaT
        rows.append({
            "title": title,
            "link": link,
            "published_at": published_dt
        })
    df = pd.DataFrame(rows)
    if df.empty:
        return df
    df["published_at"] = _coerce_utc(df["published_at"])
    # If many items, prefer within window
    if df["published_at"].notna().any():
        df = df.sort_values("published_at", ascending=False)
        window = df[df["published_at"] >= cutoff]
        if not window.empty:
            df = window
    return df

# -----------------------
# FinBERT scoring
# -----------------------
_PIPE = None
def _get_pipe():
    global _PIPE
    if _PIPE is not None:
        return _PIPE
    # Lazy import to keep cold start low
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    tok = AutoTokenizer.from_pretrained(FINBERT_MODEL)
    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)
    _PIPE = pipeline("text-classification", model=mdl, tokenizer=tok, truncation=True)
    return _PIPE

def _label_to_base(label: str) -> float:
    lab = (label or "").upper()
    if "POS" in lab:
        return 1.0
    if "NEG" in lab:
        return -1.0
    return 0.0  # neutral

def _score_finbert(texts: Sequence[str]) -> List[float]:
    """Return list of [-1,1] scores computed as (base_label in {-1,0,1}) × probability."""
    texts = [t for t in texts if isinstance(t, str) and t.strip()]
    if not texts:
        return []
    pipe = _get_pipe()
    # safeguard very large batches on first run
    texts = texts[:MAX_TOTAL_TEXTS]
    out = pipe(texts, truncation=True)
    scores = []
    for it in out:
        base = _label_to_base(it.get("label", ""))
        prob = float(it.get("score", 1.0))
        scores.append(base * prob)
    return scores

# -----------------------
# Time-decay aggregation
# -----------------------
def _exp_decay_weight(dt_series: pd.Series, now: datetime, half_life_days: float) -> pd.Series:
    """
    Compute exponential decay weights with half-life in days:
        w = 0.5 ** (age_days / half_life_days)
    """
    age = (now - dt_series).dt.total_seconds() / 86400.0
    age = age.clip(lower=0).fillna(0.0)
    hl  = max(1e-6, float(half_life_days))
    w   = pd.Series(0.5) ** (age / hl)
    return w

def _aggregate_decay(df: pd.DataFrame, half_life_days: float = HALF_LIFE_DAYS) -> Tuple[float, int]:
    """
    Weighted mean of per-headline scores with exponential time-decay.
    Returns (sentiment, count).
    """
    if df.empty:
        return 0.5, 0  # neutral when nothing
    if "score" not in df.columns or df["score"].isna().all():
        return 0.5, 0
    df = df.dropna(subset=["score", "published_at"]).copy()
    if df.empty:
        return 0.5, 0
    now = _now_utc()
    w = _exp_decay_weight(df["published_at"], now, half_life_days)
    wsum = float(w.sum())
    if wsum <= 0:
        return 0.5, 0
    s = float((df["score"] * w).sum()) / wsum
    # s is already in [-1,1]; you later normalize to [0,1] if needed.
    # Here we keep your convention of returning it centered at 0 (then you can map 0.5 as neutral if desired).
    count = int(len(df))
    # If you prefer [0,1] at this stage: s = 0.5 * (s + 1)
    return s, count

# -----------------------
# Main entry: fetch + score + aggregate for many tickers
# -----------------------
def fetch_and_score_sentiment(
    tickers_names: Sequence[Tuple[str, str]],
    primary_hours: int = 48,
    fallback_hours: int = 7*24,
    print_headlines: int = 0,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    For each (ticker, name), fetch Google News, keep up to PER_TICKER_MAX_ROWS most recent,
    score with FinBERT, and compute a time-decayed aggregate.

    Returns:
      aggregates_df: columns = ['Ticker','Sentiment','Count']
      articles_df:   columns = ['Ticker','published_at','title','score','link']
    """
    agg_rows: List[dict] = []
    art_frames: List[pd.DataFrame] = []

    for (ticker, name) in tickers_names:
        tkr = (ticker or "").strip().upper()
        nm  = (name or "").strip()

        # 1) fetch recent
        q = _build_query(tkr, nm)
        df = _google_news_rss(q, since_hours=primary_hours, locale=DEFAULT_LOCALE)
        if df.empty:
            # fallback to 7 days
            df = _google_news_rss(q, since_hours=fallback_hours, locale=DEFAULT_LOCALE)

        if df.empty:
            # no headlines → neutral fallback
            agg_rows.append({"Ticker": tkr, "Sentiment": 0.5, "Count": 0})
            time.sleep(POLITE_DELAY_SEC)
            continue

        # 2) keep newest N rows
        df = df.sort_values("published_at", ascending=False).head(PER_TICKER_MAX_ROWS).copy()

        # 3) score with FinBERT (title only, same as your code)
        texts = df["title"].fillna("").tolist()
        scores = _score_finbert(texts)
        # align length in rare cases
        if len(scores) != len(df):
            # pad or trim
            if len(scores) < len(df):
                scores = scores + [0.0] * (len(df) - len(scores))
            else:
                scores = scores[:len(df)]
        df["score"] = scores

        if print_headlines > 0:
            print(f"\n[{tkr}] {len(df)} headlines:")
            for i, r in df.head(print_headlines).iterrows():
                print(f"  - {r['published_at']:%Y-%m-%d %H:%M} | {r['title']} | score={r['score']:.3f}")

        # 4) aggregate time-decayed score
        s, n = _aggregate_decay(df[["published_at", "score"]])

        agg_rows.append({"Ticker": tkr, "Sentiment": s, "Count": n})

        # 5) attach ticker + keep minimal columns you use downstream
        df_out = df[["published_at", "title", "score"]].copy()
        df_out.insert(0, "Ticker", tkr)
        # we can include a link if available (kept in _google_news_rss)
        if "link" in df.columns:
            df_out["link"] = df["link"].values
        else:
            df_out["link"] = ""
        art_frames.append(df_out)

        # Be polite to RSS
        time.sleep(POLITE_DELAY_SEC)

    aggregates = pd.DataFrame(agg_rows, columns=["Ticker", "Sentiment", "Count"])
    articles = pd.concat(art_frames, ignore_index=True) if art_frames else pd.DataFrame(
        columns=["Ticker", "published_at", "title", "score", "link"]
    )

    # Ensure types
    if not articles.empty:
        articles["published_at"] = _coerce_utc(articles["published_at"])

    return aggregates, articles
